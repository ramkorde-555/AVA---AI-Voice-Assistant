{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "875a4622-f15a-4f15-b8f0-e24483045aef",
   "metadata": {},
   "source": [
    "# AI Voice Assistant (AVA)\n",
    "### Powered by Open Source models\n",
    "AVA uses the following pipeline:\n",
    "- Read and transcribe input audio with PyAudio and OpenAI Whisper (Query Ingestion)\n",
    "- Generate text response with Qwen Chat (Text Generation)\n",
    "- Give audio response with Bark TTS (Response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcc91d0b-280f-464c-8f3d-dd112b9cecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import sys\n",
    "import gc\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import wave\n",
    "import pyaudio\n",
    "from datasets import Dataset, Audio\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import nltk\n",
    "import numpy as np\n",
    "from bark.generation import (\n",
    "    generate_text_semantic,\n",
    "    preload_models,\n",
    ")\n",
    "from bark.api import semantic_to_waveform\n",
    "from bark import generate_audio, SAMPLE_RATE\n",
    "from scipy.io import wavfile\n",
    "from IPython import display\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"SUNO_USE_SMALL_MODELS\"] = \"0\"\n",
    "os.environ[\"SUNO_OFFLOAD_CPU\"] = \"1\"\n",
    "preload_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1696b748-e515-4047-b978-2d5a281a9fb6",
   "metadata": {},
   "source": [
    "#### Query Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0866916-9381-43aa-9fca-9e9a79c35135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_query(query_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Records user query at the sampling rate of 16000 and stores it as a .wav file\n",
    "    Args:\n",
    "    query_id: A unique id for user query\n",
    "\n",
    "    Returns:\n",
    "    Name of the recorded audio file\n",
    "    \"\"\"\n",
    "    filename = query_id + \"_Q.wav\"\n",
    "    \n",
    "    CHUNK = 1024\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    CHANNELS = 1 if sys.platform == 'darwin' else 2\n",
    "    RATE = 16000\n",
    "    RECORD_SECONDS = 10\n",
    "\n",
    "    with wave.open(filename, 'wb') as wf:\n",
    "        p = pyaudio.PyAudio()\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "    \n",
    "        stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True)\n",
    "    \n",
    "        print('Recording...')\n",
    "        for _ in range(0, RATE // CHUNK * RECORD_SECONDS):\n",
    "            wf.writeframes(stream.read(CHUNK))\n",
    "        print('Done')\n",
    "    \n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "    \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1570f163-eecb-4417-8c90-9fd73d6c08da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_query(filename: str, preloaded: tuple|None=None) -> str|None:\n",
    "    \"\"\"\n",
    "    Reads and transcribes an audio file\n",
    "    Args:\n",
    "    filename: Path to the audio recording (must be a .wav file)\n",
    "    preloaded: defines whether the model has been preloaded or not.\n",
    "    For faster responses, preload the model in the main pipeline. In this case, pass a\n",
    "    tuple of the loaded model as (processor, model).\n",
    "    To minimize memory usage, pass preloaded=None. This will load the model inside this function\n",
    "    and release it from the memory once transcription is generated. This is slower but memory efficient.\n",
    "\n",
    "    Returns:\n",
    "    Transcription of the audio file\n",
    "    \"\"\"\n",
    "\n",
    "    if not filename.endswith('.wav'):\n",
    "        print(\"Only .wav files are supported currently.\")\n",
    "        return None\n",
    "        \n",
    "    audio_dataset = Dataset.from_dict({\"audio\": [filename]}).cast_column(\"audio\", Audio())\n",
    "    audio_sample = audio_dataset[0][\"audio\"]\n",
    "    waveform = audio_sample[\"array\"]\n",
    "    sampling_rate = audio_sample[\"sampling_rate\"]\n",
    "\n",
    "    if not preloaded:\n",
    "        #Load OpenAI Whisper for transcription\n",
    "        processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\")\n",
    "        model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium\")\n",
    "        model.config.forced_decoder_ids = None\n",
    "    else:\n",
    "        processor, model = preloaded\n",
    "\n",
    "    input_features = processor(waveform, sampling_rate=sampling_rate, return_tensors=\"pt\").input_features\n",
    "    predicted_ids = model.generate(input_features)\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "    if not preloaded:\n",
    "        del processor\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return transcription[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec22b9b-d15e-4f3c-a5b0-ba8e28458c20",
   "metadata": {},
   "source": [
    "#### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52529897-142b-41f9-89f7-418055a8606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query: str, preloaded: tuple|None=None) -> str:\n",
    "    \"\"\"\n",
    "    Generates a text response for user query using Qwen-chat model.\n",
    "    Args:\n",
    "    query: User query\n",
    "    preloaded: defines whether the model has been preloaded or not.\n",
    "    For faster responses, preload the model in the main pipeline. In this case, pass a\n",
    "    tuple of the loaded model as (model, tokenizer).\n",
    "    To minimize memory usage, pass preloaded=None. This will load the model inside this function\n",
    "    and release it from the memory once transcription is generated. This is slower but memory efficient.\n",
    "    \n",
    "    Returns:\n",
    "    LLM generated response\n",
    "    \"\"\"\n",
    "\n",
    "    if not preloaded:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"Qwen/Qwen1.5-4B-Chat\",\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-4B-Chat\")\n",
    "    else:\n",
    "        model, tokenizer = preloaded\n",
    "\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant designated to respond to query to the best of your knowledge.\"},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    if not preloaded:\n",
    "        del tokenizer\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530669fa-cf86-47c4-8547-4c7d9b51a7f4",
   "metadata": {},
   "source": [
    "#### Audio Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92659653-9ee5-4a29-b1ba-deef5eb89cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_response(query_id:str, text: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts input text to speech and saves it to .wav file\n",
    "    Args:\n",
    "    query_id: Unique ID for user query\n",
    "    text: text to be converted to speech\n",
    "    \n",
    "    Returns:\n",
    "    Name of the file with the generated audio\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    GEN_TEMP = 0.6\n",
    "    SPEAKER = \"v2/en_speaker_9\"\n",
    "    silence = np.zeros(int(0.25 * SAMPLE_RATE))  # quarter second of silence\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    pieces = []\n",
    "    for sentence in sentences:\n",
    "        semantic_tokens = generate_text_semantic(\n",
    "            sentence,\n",
    "            history_prompt=SPEAKER,\n",
    "            temp=GEN_TEMP,\n",
    "            min_eos_p=0.05,  # this controls how likely the generation is to end\n",
    "        )\n",
    "    \n",
    "        audio_array = semantic_to_waveform(semantic_tokens, history_prompt=SPEAKER,)\n",
    "        pieces += [audio_array, silence.copy()]\n",
    "\n",
    "    waveform = np.concatenate(pieces)\n",
    "    audio = display.Audio(waveform, rate=SAMPLE_RATE)\n",
    "    filename = query_id + \"_A.wav\"\n",
    "    with open(filename, \"wb\") as file:\n",
    "          file.write(audio.data)\n",
    "    #wavfile.write(filename, SAMPLE_RATE, waveform.astype(np.dtype('i2')))\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae01a9b-55da-4b3f-b4da-d450666f6dae",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9375703-24c6-490d-9d06-304374ea3d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceAssistant:\n",
    "    def __init__(self, preload:list|None=None):\n",
    "        self.preload = preload\n",
    "        if not self.preload:\n",
    "            self.preloaded_stt = None\n",
    "            self.preloaded_tgen = None\n",
    "        else:\n",
    "            self.preload = [mode.lower() for mode in self.preload]\n",
    "            if 'transcribe' in self.preload():\n",
    "                self.processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\")\n",
    "                self.stt_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium\")\n",
    "                self.stt_model.config.forced_decoder_ids = None\n",
    "                self.preloaded_stt = (self.processor, self.stt_model)\n",
    "            else:\n",
    "                self.preloaded_stt = None\n",
    "\n",
    "            if 'response' in self.preload():\n",
    "                self.response_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    \"Qwen/Qwen1.5-4B-Chat\",\n",
    "                    torch_dtype=\"auto\",\n",
    "                    device_map=\"auto\"\n",
    "                )\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-4B-Chat\")\n",
    "                self.preloaded_tgen = (self.response_model, self.tokenizer)\n",
    "            else:\n",
    "                self.preloaded_tgen = None\n",
    "\n",
    "    def run(self, query_id):\n",
    "        self.recorded_file = record_query(query_id)\n",
    "        print(\"Transcribing the audio...\")\n",
    "        self.transcription = read_query(self.recorded_file, preloaded=self.preloaded_stt)\n",
    "        print(\"Generating response...\")\n",
    "        self.response = generate_response(self.transcription, preloaded=self.preloaded_tgen)\n",
    "        print(\"Storing results...\")\n",
    "        self.target_file = audio_response(query_id, self.response)\n",
    "        print(\"Done. The output is stored in\", self.target_file)\n",
    "        return #self.target_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e59d1764-9671-4021-b638-5c70bed9afc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Done\n",
      "Transcribing the audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.90it/s]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 75/75 [00:02<00:00, 32.14it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:06<00:00,  1.68s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 605/605 [00:20<00:00, 29.26it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:53<00:00,  1.73s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 632/632 [00:18<00:00, 33.95it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 32/32 [3:42:11<00:00, 416.62s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 636/636 [00:37<00:00, 17.13it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [00:44<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. The output is stored in test01_A.wav\n"
     ]
    }
   ],
   "source": [
    "AVA = VoiceAssistant()\n",
    "query_id = \"test01\"\n",
    "AVA.run(query_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f72d75-58a9-48d4-bc4e-b4695d5bfa89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
